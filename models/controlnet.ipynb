{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c29bd31-f103-4879-b8fe-53592284b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlNetModel(\n",
      "  (time_proj): Timesteps()\n",
      "  (time_embedding): TimestepEmbedding(\n",
      "    (linear_1): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (act): SiLU()\n",
      "    (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (A_embedding): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): GroupNorm(2, 128, eps=1e-05, affine=True)\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (B_embedding): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): GroupNorm(2, 128, eps=1e-05, affine=True)\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (attn0): CBAMLayer(\n",
      "    (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (mlp): Sequential(\n",
      "      (0): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    )\n",
      "    (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (A_down_res): ModuleList(\n",
      "    (0): ResnetBlock2D(\n",
      "      (norm1): GroupNorm(4, 128, eps=1e-06, affine=True)\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (time_emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (norm2): GroupNorm(4, 128, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "    (1): ResnetBlock2D(\n",
      "      (norm1): GroupNorm(8, 128, eps=1e-06, affine=True)\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (time_emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (norm2): GroupNorm(8, 256, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "      (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (A_down_sample): ModuleList(\n",
      "    (0): Downsample2D(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (1): Downsample2D(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (attn): ModuleList(\n",
      "    (0): CBAMLayer(\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (mlp): Sequential(\n",
      "        (0): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (1): CBAMLayer(\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (mlp): Sequential(\n",
      "        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "      (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (B_down_res): ModuleList(\n",
      "    (0): ResnetBlock2D(\n",
      "      (norm1): GroupNorm(4, 128, eps=1e-06, affine=True)\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (time_emb_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (norm2): GroupNorm(4, 128, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "    (1): ResnetBlock2D(\n",
      "      (norm1): GroupNorm(8, 128, eps=1e-06, affine=True)\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (time_emb_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (norm2): GroupNorm(8, 256, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "      (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (B_down_sample): ModuleList(\n",
      "    (0): Downsample2D(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (1): Downsample2D(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (mid_convs): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
      "    )\n",
      "    (1): Conv2d(256, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "tensor([0.5500], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/diffusers/models/downsampling.py:135: FutureWarning: `scale` is deprecated and will be removed in version 1.0.0. The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\n",
      "  deprecate(\"scale\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'out': tensor([[[[-5.0169e-01,  3.1622e-01,  1.4678e-02,  ..., -1.0927e-01,\n",
       "            -1.6955e-02,  2.1171e-01],\n",
       "           [-3.7797e-01,  5.6160e-01, -4.7429e-02,  ..., -9.1338e-02,\n",
       "            -1.4452e-01, -6.3993e-04],\n",
       "           [ 6.4609e-01,  2.3995e-02,  1.0878e-01,  ..., -3.3849e-01,\n",
       "             4.2445e-01, -3.5544e-01],\n",
       "           ...,\n",
       "           [ 5.1249e-01,  4.9295e-01, -6.3349e-01,  ...,  4.7150e-01,\n",
       "             2.7074e-01,  3.3534e-01],\n",
       "           [ 4.7156e-01,  1.1992e+00,  6.3551e-01,  ...,  1.0864e+00,\n",
       "             1.7913e-01, -2.1629e-01],\n",
       "           [ 5.6860e-01,  4.1762e-02,  6.2006e-01,  ...,  4.4022e-01,\n",
       "             8.5035e-01,  1.7030e-02]],\n",
       " \n",
       "          [[-5.1609e-02,  6.1552e-01, -4.4772e-01,  ..., -4.7968e-02,\n",
       "             1.7964e-01,  6.0823e-02],\n",
       "           [ 9.0007e-02, -3.7348e-01, -7.6118e-01,  ...,  7.4168e-02,\n",
       "            -2.0555e-01,  5.4312e-01],\n",
       "           [ 4.5340e-01, -5.9688e-01, -6.7501e-01,  ...,  3.3553e-01,\n",
       "            -3.0319e-01,  4.3448e-01],\n",
       "           ...,\n",
       "           [-2.3161e-01, -2.1316e-02,  3.6156e-01,  ..., -6.3323e-01,\n",
       "             1.1829e-01,  4.3889e-01],\n",
       "           [ 7.4875e-01,  5.1384e-02, -3.5677e-01,  ..., -8.2613e-02,\n",
       "             2.6643e-01,  4.2232e-01],\n",
       "           [ 7.7948e-01,  1.1390e-01, -3.2373e-01,  ..., -7.9643e-01,\n",
       "            -4.5646e-01, -5.0437e-01]],\n",
       " \n",
       "          [[-2.5845e-01, -4.4810e-01, -5.7714e-01,  ...,  2.1256e-01,\n",
       "             4.6249e-01,  2.8285e-02],\n",
       "           [-9.9268e-01,  4.1288e-01,  8.7595e-02,  ..., -1.8864e-01,\n",
       "             5.2347e-01,  3.3620e-01],\n",
       "           [-4.1273e-01,  5.7210e-01,  1.5681e+00,  ...,  4.0644e-02,\n",
       "             4.6328e-01, -3.4307e-01],\n",
       "           ...,\n",
       "           [-6.9800e-01, -6.3223e-01, -2.9219e-01,  ...,  1.2929e-01,\n",
       "             1.3539e-01, -3.9396e-01],\n",
       "           [-1.3728e-02, -3.9191e-01, -2.8331e-01,  ..., -4.4850e-01,\n",
       "             5.2881e-01,  1.4753e-01],\n",
       "           [ 1.8991e-01, -1.3889e+00,  9.0705e-02,  ..., -6.3620e-01,\n",
       "            -1.0410e-01, -2.4851e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-9.2885e-03,  2.7614e-01, -4.9881e-02,  ...,  3.7752e-01,\n",
       "             3.9744e-01,  1.6992e-02],\n",
       "           [ 7.7925e-02,  8.2177e-01,  8.4020e-01,  ...,  8.5443e-01,\n",
       "            -2.3980e-01,  3.6911e-01],\n",
       "           [-1.2753e+00,  4.7081e-01,  3.8826e-01,  ...,  3.3161e-01,\n",
       "             1.3279e-01,  5.5572e-02],\n",
       "           ...,\n",
       "           [ 2.9586e-01,  9.0824e-01,  1.0998e+00,  ..., -3.3631e-01,\n",
       "             1.3769e-01, -1.2932e-01],\n",
       "           [-6.4855e-01, -3.4514e-02,  6.0045e-01,  ...,  6.8412e-01,\n",
       "             2.1745e-01,  6.4077e-01],\n",
       "           [-3.1803e-01,  2.8027e-01,  5.7404e-01,  ...,  1.4756e-02,\n",
       "             2.5127e-01, -2.8752e-02]],\n",
       " \n",
       "          [[ 2.2696e-01,  7.4197e-01, -5.1369e-01,  ..., -4.4536e-03,\n",
       "             6.5543e-02,  4.1726e-02],\n",
       "           [ 6.6924e-03, -1.6629e-01, -5.5624e-02,  ..., -2.6522e-01,\n",
       "             1.7785e-01,  1.3999e-01],\n",
       "           [ 7.0235e-02,  2.8984e-01, -1.9859e-01,  ..., -1.2758e-01,\n",
       "            -3.5088e-01,  1.9462e-02],\n",
       "           ...,\n",
       "           [ 2.4140e-01,  5.0885e-01, -9.7200e-01,  ...,  1.2426e-01,\n",
       "            -4.7135e-01, -4.8911e-01],\n",
       "           [ 6.4379e-02, -1.3543e-01,  4.7411e-01,  ...,  1.5402e-01,\n",
       "            -8.4257e-02,  1.4643e-01],\n",
       "           [-1.0440e-01, -3.0127e-01,  1.9268e-02,  ..., -9.8493e-02,\n",
       "             2.5907e-01,  1.0871e-01]],\n",
       " \n",
       "          [[ 5.3390e-02, -4.2899e-02,  4.8859e-01,  ...,  7.4651e-02,\n",
       "             3.3813e-01, -9.4334e-02],\n",
       "           [ 8.4860e-01,  1.7718e-01,  8.9583e-01,  ..., -3.5469e-02,\n",
       "             4.7345e-01, -2.6475e-02],\n",
       "           [ 8.9693e-01,  8.0075e-01,  2.3013e-01,  ...,  4.9258e-01,\n",
       "             1.4582e-01, -2.1257e-01],\n",
       "           ...,\n",
       "           [ 2.8525e-01, -5.7573e-02, -6.1207e-02,  ...,  9.5223e-01,\n",
       "             8.4247e-02, -1.1965e-01],\n",
       "           [-3.0505e-01, -1.3335e-01,  2.8176e-01,  ..., -4.9914e-01,\n",
       "            -2.0020e-01, -5.8832e-02],\n",
       "           [ 3.7785e-02,  5.0561e-01, -6.3426e-01,  ...,  4.4950e-01,\n",
       "             2.9465e-01,  4.2106e-02]]]], device='cuda:0',\n",
       "        grad_fn=<ConvolutionBackward0>),\n",
       " 'scale': 1.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright 2023 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.utils import BaseOutput, logging\n",
    "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.resnet import Downsample2D, ResnetBlock2D\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ControlNetOutput(BaseOutput):\n",
    "    \"\"\"\n",
    "    The output of [`ControlNetModel`].\n",
    "\n",
    "    Args:\n",
    "        down_block_res_samples (`tuple[torch.Tensor]`):\n",
    "            A tuple of downsample activations at different resolutions for each downsampling block. Each tensor should\n",
    "            be of shape `(batch_size, channel * resolution, height //resolution, width // resolution)`. Output can be\n",
    "            used to condition the original UNet's downsampling activations.\n",
    "        mid_down_block_re_sample (`torch.Tensor`):\n",
    "            The activation of the midde block (the lowest sample resolution). Each tensor should be of shape\n",
    "            `(batch_size, channel * lowest_resolution, height // lowest_resolution, width // lowest_resolution)`.\n",
    "            Output can be used to condition the original UNet's middle block activation.\n",
    "    \"\"\"\n",
    "\n",
    "    down_block_res_samples: Tuple[torch.Tensor]\n",
    "    mid_block_res_sample: torch.Tensor\n",
    "\n",
    "\n",
    "class Block2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        temb_channels: int,\n",
    "        dropout: float = 0.0,\n",
    "        num_layers: int = 1,\n",
    "        resnet_eps: float = 1e-6,\n",
    "        resnet_time_scale_shift: str = \"default\",\n",
    "        resnet_act_fn: str = \"swish\",\n",
    "        resnet_groups: int = 32,\n",
    "        resnet_pre_norm: bool = True,\n",
    "        output_scale_factor: float = 1.0,\n",
    "        add_downsample: bool = True,\n",
    "        downsample_padding: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        resnets = []\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            in_channels = in_channels if i == 0 else out_channels\n",
    "            resnets.append(\n",
    "                ResnetBlock2D(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    temb_channels=temb_channels,\n",
    "                    eps=resnet_eps,\n",
    "                    groups=resnet_groups,\n",
    "                    dropout=dropout,\n",
    "                    time_embedding_norm=resnet_time_scale_shift,\n",
    "                    non_linearity=resnet_act_fn,\n",
    "                    output_scale_factor=output_scale_factor,\n",
    "                    pre_norm=resnet_pre_norm,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.resnets = nn.ModuleList(resnets)\n",
    "\n",
    "        if add_downsample:\n",
    "            self.downsamplers = nn.ModuleList(\n",
    "                [\n",
    "                    Downsample2D(\n",
    "                        out_channels,\n",
    "                        use_conv=True,\n",
    "                        out_channels=out_channels,\n",
    "                        padding=downsample_padding,\n",
    "                        name=\"op\",\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.downsamplers = None\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        temb: Optional[torch.FloatTensor] = None,\n",
    "    ) -> Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]:\n",
    "        output_states = ()\n",
    "\n",
    "        for resnet in zip(self.resnets):\n",
    "            hidden_states = resnet(hidden_states, temb)\n",
    "            output_states += (hidden_states,)\n",
    "\n",
    "        if self.downsamplers is not None:\n",
    "            for downsampler in self.downsamplers:\n",
    "                hidden_states = downsampler(hidden_states)\n",
    "\n",
    "            output_states += (hidden_states,)\n",
    "\n",
    "        return hidden_states, output_states\n",
    "\n",
    "\n",
    "class IdentityModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IdentityModule, self).__init__()\n",
    "\n",
    "    def forward(self, *args):\n",
    "        if len(args) > 0:\n",
    "            return args[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: Optional[int] = None,\n",
    "                 stride=1,\n",
    "                 conv_shortcut: bool = False,\n",
    "                 dropout: float = 0.0,\n",
    "                 temb_channels: int = 512,\n",
    "                 groups: int = 32,\n",
    "                 groups_out: Optional[int] = None,\n",
    "                 pre_norm: bool = True,\n",
    "                 eps: float = 1e-6,\n",
    "                 non_linearity: str = \"swish\",\n",
    "                 skip_time_act: bool = False,\n",
    "                 time_embedding_norm: str = \"default\",  # default, scale_shift, ada_group, spatial\n",
    "                 kernel: Optional[torch.FloatTensor] = None,\n",
    "                 output_scale_factor: float = 1.0,\n",
    "                 use_in_shortcut: Optional[bool] = None,\n",
    "                 up: bool = False,\n",
    "                 down: bool = False,\n",
    "                 conv_shortcut_bias: bool = True,\n",
    "                 conv_2d_out_channels: Optional[int] = None,):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels,\n",
    "                          out_channels,\n",
    "                          kernel_size=3 if stride != 1 else 1,\n",
    "                          stride=stride,\n",
    "                          padding=1 if stride != 1 else 0,\n",
    "                          bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block2D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        temb_channels: int,\n",
    "        dropout: float = 0.0,\n",
    "        num_layers: int = 1,\n",
    "        resnet_eps: float = 1e-6,\n",
    "        resnet_time_scale_shift: str = \"default\",\n",
    "        resnet_act_fn: str = \"swish\",\n",
    "        resnet_groups: int = 32,\n",
    "        resnet_pre_norm: bool = True,\n",
    "        output_scale_factor: float = 1.0,\n",
    "        add_downsample: bool = True,\n",
    "        downsample_padding: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        resnets = []\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            # in_channels = in_channels if i == 0 else out_channels\n",
    "            resnets.append(\n",
    "                # ResnetBlock2D(\n",
    "                #     in_channels=in_channels,\n",
    "                #     out_channels=out_channels,\n",
    "                #     temb_channels=temb_channels,\n",
    "                #     eps=resnet_eps,\n",
    "                #     groups=resnet_groups,\n",
    "                #     dropout=dropout,\n",
    "                #     time_embedding_norm=resnet_time_scale_shift,\n",
    "                #     non_linearity=resnet_act_fn,\n",
    "                #     output_scale_factor=output_scale_factor,\n",
    "                #     pre_norm=resnet_pre_norm,\n",
    "                BasicBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    temb_channels=temb_channels,\n",
    "                    eps=resnet_eps,\n",
    "                    groups=resnet_groups,\n",
    "                    dropout=dropout,\n",
    "                    time_embedding_norm=resnet_time_scale_shift,\n",
    "                    non_linearity=resnet_act_fn,\n",
    "                    output_scale_factor=output_scale_factor,\n",
    "                    pre_norm=resnet_pre_norm,\n",
    "                ) if i == num_layers - 1 else \\\n",
    "                IdentityModule()\n",
    "            )\n",
    "\n",
    "        self.resnets = nn.ModuleList(resnets)\n",
    "\n",
    "        if add_downsample:\n",
    "            self.downsamplers = nn.ModuleList(\n",
    "                [\n",
    "                    # Downsample2D(\n",
    "                    #     out_channels,\n",
    "                    #     use_conv=True,\n",
    "                    #     out_channels=out_channels,\n",
    "                    #     padding=downsample_padding,\n",
    "                    #     name=\"op\",\n",
    "                    # )\n",
    "                    BasicBlock(\n",
    "                        in_channels=out_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        temb_channels=temb_channels,\n",
    "                        stride=2,\n",
    "                        eps=resnet_eps,\n",
    "                        groups=resnet_groups,\n",
    "                        dropout=dropout,\n",
    "                        time_embedding_norm=resnet_time_scale_shift,\n",
    "                        non_linearity=resnet_act_fn,\n",
    "                        output_scale_factor=output_scale_factor,\n",
    "                        pre_norm=resnet_pre_norm,\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.downsamplers = None\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        temb: Optional[torch.FloatTensor] = None,\n",
    "    ) -> Union[torch.FloatTensor, Tuple[torch.FloatTensor, ...]]:\n",
    "        output_states = ()\n",
    "\n",
    "        for resnet in self.resnets:\n",
    "            hidden_states = resnet(hidden_states, temb)\n",
    "            output_states += (hidden_states,)\n",
    "\n",
    "        if self.downsamplers is not None:\n",
    "            for downsampler in self.downsamplers:\n",
    "                hidden_states = downsampler(hidden_states)\n",
    "\n",
    "            output_states += (hidden_states,)\n",
    "\n",
    "        return hidden_states, output_states\n",
    "\n",
    "\n",
    "class ControlProject(nn.Module):\n",
    "    def __init__(self, num_channels, scale=8, is_empty=False) -> None:\n",
    "        super().__init__()\n",
    "        assert scale and scale & (scale - 1) == 0\n",
    "        self.is_empty = is_empty\n",
    "        self.scale = scale\n",
    "        if not is_empty:\n",
    "            if scale > 1:\n",
    "                self.down_scale = nn.AvgPool2d(scale, scale)\n",
    "            else:\n",
    "                self.down_scale = nn.Identity()\n",
    "            self.out = nn.Conv2d(num_channels, num_channels, kernel_size=1, stride=1, bias=False)\n",
    "            for p in self.out.parameters():\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states: torch.FloatTensor):\n",
    "        if self.is_empty:\n",
    "            shape = list(hidden_states.shape)\n",
    "            shape[-2] = shape[-2] // self.scale\n",
    "            shape[-1] = shape[-1] // self.scale\n",
    "            return torch.zeros(shape).to(hidden_states)\n",
    "\n",
    "        if len(hidden_states.shape) == 5:\n",
    "            B, F, C, H, W = hidden_states.shape\n",
    "            hidden_states = rearrange(hidden_states, \"B F C H W -> (B F) C H W\")\n",
    "            hidden_states = self.down_scale(hidden_states)\n",
    "            hidden_states = self.out(hidden_states)\n",
    "            hidden_states = rearrange(hidden_states, \"(B F) C H W -> B F C H W\", F=F)\n",
    "        else:\n",
    "            hidden_states = self.down_scale(hidden_states)\n",
    "            hidden_states = self.out(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class CBAMLayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16, spatial_kernel=7):\n",
    "        super(CBAMLayer, self).__init__()\n",
    "\n",
    "        # channel attention 压缩H,W为1\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # shared MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            # Conv2d比Linear方便操作\n",
    "            # nn.Linear(channel, channel // reduction, bias=False)\n",
    "            nn.Conv2d(channel, channel // reduction, 1, bias=False),\n",
    "            # inplace=True直接替换，节省内存\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(channel // reduction, channel,bias=False)\n",
    "            nn.Conv2d(channel // reduction, channel, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        # spatial attention\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=spatial_kernel,\n",
    "                              padding=spatial_kernel // 2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        max_out = self.mlp(self.max_pool(B))\n",
    "        avg_out = self.mlp(self.avg_pool(B))\n",
    "        channel_out = self.sigmoid(max_out + avg_out)\n",
    "        A = channel_out * A\n",
    "\n",
    "        max_out, _ = torch.max(B, dim=1, keepdim=True)\n",
    "        avg_out = torch.mean(B, dim=1, keepdim=True)\n",
    "        spatial_out = self.sigmoid(self.conv(torch.cat([max_out, avg_out], dim=1)))\n",
    "        A = spatial_out * A\n",
    "        return A\n",
    "\n",
    "class ControlNetModel(ModelMixin, ConfigMixin):\n",
    "\n",
    "    _supports_gradient_checkpointing = True\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: List[int] = [128, 128],\n",
    "        out_channels: List[int] = [128, 256],\n",
    "        groups: List[int] = [4, 8],\n",
    "        time_embed_dim: int = 256,\n",
    "        final_out_channels: int = 384,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_proj = Timesteps(128, True, downscale_freq_shift=0)\n",
    "        self.time_embedding = TimestepEmbedding(128, time_embed_dim)\n",
    "\n",
    "        self.A_embedding = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(2, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.B_embedding = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.GroupNorm(2, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.attn0 = CBAMLayer(channel=128)\n",
    "\n",
    "        self.A_down_res = nn.ModuleList()\n",
    "        self.A_down_sample = nn.ModuleList()\n",
    "        self.attn = nn.ModuleList()\n",
    "        for i in range(len(in_channels)):\n",
    "            self.A_down_res.append(\n",
    "                ResnetBlock2D(\n",
    "                    in_channels=in_channels[i],\n",
    "                    out_channels=out_channels[i],\n",
    "                    temb_channels=time_embed_dim,\n",
    "                    groups=groups[i]\n",
    "                ),\n",
    "            )\n",
    "            self.A_down_sample.append(\n",
    "                Downsample2D(\n",
    "                    out_channels[i],\n",
    "                    use_conv=True,\n",
    "                    out_channels=out_channels[i],\n",
    "                    padding=1,\n",
    "                    name=\"op\",\n",
    "                )\n",
    "            )\n",
    "            self.attn.append(\n",
    "                CBAMLayer(out_channels[i])\n",
    "            )\n",
    "\n",
    "        self.B_down_res = nn.ModuleList()\n",
    "        self.B_down_sample = nn.ModuleList()\n",
    "        for i in range(len(in_channels)):\n",
    "            self.B_down_res.append(\n",
    "                ResnetBlock2D(\n",
    "                    in_channels=in_channels[i],\n",
    "                    out_channels=out_channels[i],\n",
    "                    temb_channels=time_embed_dim,\n",
    "                    groups=groups[i]\n",
    "                ),\n",
    "            )\n",
    "            self.B_down_sample.append(\n",
    "                Downsample2D(\n",
    "                    out_channels[i],\n",
    "                    use_conv=True,\n",
    "                    out_channels=out_channels[i],\n",
    "                    padding=1,\n",
    "                    name=\"op\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.mid_convs = nn.ModuleList()\n",
    "        self.mid_convs.append(nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels[-1],\n",
    "                out_channels=out_channels[-1],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.GroupNorm(8, out_channels[-1]),\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels[-1],\n",
    "                out_channels=out_channels[-1],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.GroupNorm(8, out_channels[-1]),\n",
    "        ))\n",
    "        self.mid_convs.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels[-1],\n",
    "                out_channels=final_out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "            ))\n",
    "        self.scale = 1.0  # nn.Parameter(torch.tensor(1.))\n",
    "\n",
    "    def _set_gradient_checkpointing(self, module, value=False):\n",
    "        if hasattr(module, \"gradient_checkpointing\"):\n",
    "            module.gradient_checkpointing = value\n",
    "\n",
    "    # Copied from diffusers.models.unet_3d_condition.UNet3DConditionModel.enable_forward_chunking\n",
    "    def enable_forward_chunking(self, chunk_size: Optional[int] = None, dim: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Sets the attention processor to use [feed forward\n",
    "        chunking](https://huggingface.co/blog/reformer#2-chunked-feed-forward-layers).\n",
    "\n",
    "        Parameters:\n",
    "            chunk_size (`int`, *optional*):\n",
    "                The chunk size of the feed-forward layers. If not specified, will run feed-forward layer individually\n",
    "                over each tensor of dim=`dim`.\n",
    "            dim (`int`, *optional*, defaults to `0`):\n",
    "                The dimension over which the feed-forward computation should be chunked. Choose between dim=0 (batch)\n",
    "                or dim=1 (sequence length).\n",
    "        \"\"\"\n",
    "        if dim not in [0, 1]:\n",
    "            raise ValueError(f\"Make sure to set `dim` to either 0 or 1, not {dim}\")\n",
    "\n",
    "        # By default chunk size is 1\n",
    "        chunk_size = chunk_size or 1\n",
    "\n",
    "        def fn_recursive_feed_forward(module: torch.nn.Module, chunk_size: int, dim: int):\n",
    "            if hasattr(module, \"set_chunk_feed_forward\"):\n",
    "                module.set_chunk_feed_forward(chunk_size=chunk_size, dim=dim)\n",
    "\n",
    "            for child in module.children():\n",
    "                fn_recursive_feed_forward(child, chunk_size, dim)\n",
    "\n",
    "        for module in self.children():\n",
    "            fn_recursive_feed_forward(module, chunk_size, dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        mask: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "    ) -> Union[ControlNetOutput, Tuple]:\n",
    "\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        batch_size = sample.shape[0]\n",
    "        timesteps = timesteps.expand(batch_size)\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "        # `Timesteps` does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=sample.dtype)\n",
    "        emb_batch = self.time_embedding(t_emb)\n",
    "\n",
    "        # Repeat the embeddings num_video_frames times\n",
    "        # emb: [batch, channels] -> [batch * frames, channels]\n",
    "        emb = emb_batch\n",
    "\n",
    "\n",
    "        sample = self.A_embedding(sample)\n",
    "        mask = self.B_embedding(mask)\n",
    "        sample = self.attn0(sample, mask)\n",
    "\n",
    "        for a_res, a_downsample ,b_res, b_downsample, attn in zip(self.A_down_res, self.A_down_sample, self.B_down_res, self.B_down_sample,self.attn):\n",
    "            sample = a_res(sample, emb)\n",
    "            sample = a_downsample(sample, emb)\n",
    "            mask = b_res(mask, emb)\n",
    "            mask = b_downsample(mask, emb)\n",
    "            sample = attn(sample, mask)\n",
    "            #TODO 对于mask是不是太过了，不用残差快，直接用卷积？\n",
    "\n",
    "\n",
    "        sample = self.mid_convs[0](sample) + sample\n",
    "        sample = self.mid_convs[1](sample)\n",
    "        return {\n",
    "            'out': sample,\n",
    "            'scale': self.scale,\n",
    "        }\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    for p in module.parameters():\n",
    "        nn.init.zeros_(p)\n",
    "    return module\n",
    "\n",
    "model = ControlNetModel().to(\"cuda\")\n",
    "\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "r1 = torch.randn(1,3, 512, 512).to(\"cuda\")\n",
    "r2 = torch.randn(1,3, 512, 512).to(\"cuda\")\n",
    "timestep = torch.randn(1).to(\"cuda\")\n",
    "print(timestep)\n",
    "model(r1,r2,timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad65728-ee0b-4828-bbe0-483aea66608e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
